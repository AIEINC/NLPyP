<!DOCTYPE html>
<html>
<head>
    <script src="https://unpkg.com/htmx.org@1.9.9"></script>
    <link rel="stylesheet" href="/static/styles.css">
    <title>NLPyP</title>
</head>
<body>
<form hx-post="/summarize" hx-target="#output">
    <textarea name="input_text" required></textareaimport os
import re
import string
import logging
import numpy as np
import nltk
import spacy
from sklearn.metrics.pairwise import cosine_similarity
from transformers import pipeline
from sentence_transformers import SentenceTransformer
from collections import Counter
from math import log2

Setup

nltk.download('punkt')
nlp = spacy.load("en_core_web_sm")
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(name)

Mock emotion lexicon (simulate NRC)

emotion_lexicon = {
"success": "joy", "win": "joy", "fear": "fear", "dark": "sadness", "light": "joy",
"control": "trust", "goal": "anticipation", "teach": "trust", "speak": "trust"
}

Utility functions

def count_syllables(word):
word = word.lower()
vowels = "aeiouy"
syllables = 0
if word[0] in vowels:
syllables += 1
for idx in range(1, len(word)):
if word[idx] in vowels and word[idx - 1] not in vowels:
syllables += 1
if word.endswith("e"):
syllables -= 1
return max(1, syllables)

def flesch_kincaid_grade(text):
words = nltk.word_tokenize(text)
syllables = sum(count_syllables(word) for word in words)
return round(0.39 * (len(words)) + 11.8 * (syllables / len(words)) - 15.59, 2)

def compute_entropy(text):
words = nltk.word_tokenize(text.lower())
freq = Counter(words)
total = sum(freq.values())
probs = [count / total for count in freq.values()]
return -sum(p * log2(p) for p in probs)

Module 1: Preprocessing

def preprocess_text(text):
doc = nlp(text)
sentences = list(doc.sents)
return [{
"text": sent.text.strip(),
"entropy": compute_entropy(sent.text),
"flesch_kincaid": flesch_kincaid_grade(sent.text),
"length": len(sent),
"named_entities": [(ent.text, ent.label_) for ent in sent.ents]
} for sent in sentences]

Module 2: Semantic Salience via Embeddings

def rank_by_semantic_salience(sentences):
all_text = " ".join(s["text"] for s in sentences)
doc_vec = embedding_model.encode([all_text])[0]
sent_vecs = embedding_model.encode([s["text"] for s in sentences])
for i, s in enumerate(sentences):
s["semantic_score"] = cosine_similarity([sent_vecs[i]], [doc_vec])[0][0]
return sorted(sentences, key=lambda s: s["semantic_score"], reverse=True)

Module 3: Abstractive Summarization

def abstract_summarize(top_sentences, summary_ratio):
combined_text = " ".join(s["text"] for s in top_sentences)
max_chars = max(60, int(len(combined_text) * summary_ratio))
summary = summarizer(combined_text, min_length=30, max_length=max_chars, do_sample=False)
return summary[0]["summary_text"]

Module 4: Optimization

def chunk_text(text, chunk_size=7):
words = text.split()
return [" ".join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]

def enhance_emotion(chunk, lexicon, tone):
enhanced = []
for word in chunk.split():
base = word.lower().strip(string.punctuation)
if base in lexicon and (tone == "emotional" or lexicon[base] == tone):
enhanced.append(word.upper())
else:
enhanced.append(word)
return " ".join(enhanced)

def simplify_structure(sentence):
sentence = re.sub(r'\bis being\b', 'is', sentence)
sentence = re.sub(r'\bhas been\b', 'had', sentence)
return sentence

def pattern_chunks(chunks):
return [chunk + "." for chunk in chunks]

def optimize_summary(text, tone="positive"):
chunks = chunk_text(text)
emotionalized = [enhance_emotion(c, emotion_lexicon, tone) for c in chunks]
simplified = [simplify_structure(c) for c in emotionalized]
patterned = pattern_chunks(simplified)
return patterned

Module 5: Output Generation

def generate_output(chunks, tier_1_words, tier_2_words):
flat_text = " ".join(chunks)
words = flat_text.split()
tier_1 = " ".join(words[:tier_1_words]).strip(".") + "."
tier_2 = " ".join(words[:tier_2_words]).strip(".") + "."
return {"tier_1": tier_1, "tier_2": tier_2}

Master Pipeline

def neuro_semantic_summarizer(text, config=None):
config = config or {
"summary_ratio": 0.2,
"tone": "positive",
"tier_1_words": 20,
"tier_2_words": 100,
"top_n": 5
}

try:  
    processed = preprocess_text(text)  
    top = rank_by_semantic_salience(processed)[:config["top_n"]]  
    abstract = abstract_summarize(top, config["summary_ratio"])  
    optimized_chunks = optimize_summary(abstract, tone=config["tone"])  
    return generate_output(optimized_chunks, config["tier_1_words"], config["tier_2_words"])  
except Exception as e:  
    logger.error(f"Summarization pipeline failed: {e}")  
    return {"tier_1": "", "tier_2": ""}

><br>
    <select name="tone">
        <option>joy</option>
        <option>fear</option>
        <option>sadness</option>
        <option>trust</option>
        <option>anticipation</option>
    </select>
    <input type="number" name="ratio" value="0.2" step="0.05" min="0.1" max="1.0">
    <button>Summarize</button>
</form>
<div id="output">{% if summary %}<h3>Tier 1:</h3><p>{{ summary.tier_1 }}</p><h3>Tier 2:</h3><p>{{ summary.tier_2 }}</p>{% endif %}</div>
</body>
</html>
